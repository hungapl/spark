{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Import data\n",
    "#data = spark.read.csv(\"books.csv\", header=True, inferSchema=True)\n",
    "#print(data.show())\n",
    "#print(data.schema)\n",
    "\n",
    "#books = data.rdd\n",
    "numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "s = [\"a\", \"b c\", \"d e f\"]\n",
    "strings = spark.sparkContext.parallelize(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "- Actions are eager - performs immediately and return results to driver program\n",
    "\n",
    "**How to tell whether a operation is an action**\n",
    "- Result of an action is not an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce()** \n",
    "- aggregate elements in the dataset using a function, \n",
    "- the function should be associative and commutative so it can be computed in parallel (**parallelizable**)\n",
    "- The function accepts two input variable (2 consecutive variables in the rdd) and output a single variable\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "numbers.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fold()**\n",
    "- Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregate()**\n",
    "- Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”\n",
    "- This function requires two operations: seqOp and combOp\n",
    "- Let's say the RDD of type T.  seqOp can produce a result of a different type U, hence the two input for seqOp are T and U. (T, U) -> U\n",
    "- Then a combine operations is required to combine the two Us into a single result.  (U, U) -> V\n",
    "- Good explanation here: https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark\n",
    "- groupbys are probably based on this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (0)\n",
    "numbers.aggregate(0, add, add)\n",
    "\n",
    "# Example that return different type\n",
    "acc = (0) =\n",
    "seqOp = (lambda acc, s: acc + len(s.split(' ')))\n",
    "combOp = (lambda acc1, acc2: acc1 + acc2) # combine accumulator values from all partitions\n",
    "strings.aggregate(acc, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of other useful actions:\n",
    "\n",
    "**Number of elements in dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return first n elements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where's Spot\n",
      "First 2 books:\n",
      "[Row(title=\"Where's Spot\", author='Eric Hill', type='Children', price=10), Row(title='The Cat In The Hat', author='Dr. Seuss', type='Children', price=15)]\n"
     ]
    }
   ],
   "source": [
    "print(books.first().title)\n",
    "print(\"First 2 books:\")\n",
    "print(str(books.take(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving objects**\n",
    "e.g. saveAsTextFile\n",
    "(saveAsSequenceFile and saveAsObjectFile are only available in Java and Scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save as text file\n",
    "rdd.saveAsTextFile(\"./_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
