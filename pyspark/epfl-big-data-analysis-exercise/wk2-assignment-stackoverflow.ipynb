{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering StackOverflow Q&A\n",
    "\n",
    "EPFL Big Data Analysis Week 2 Assignment\n",
    "https://www.coursera.org/learn/scala-spark-big-data/home/info\n",
    "\n",
    "\"The overall goal of this assignment is to implement a distributed k-means algorithm which clusters posts on the popular question-answer platform StackOverflow according to their score. Moreover, this clustering should be executed in parallel for different programming languages, and the results should be compared.\n",
    "\n",
    "The motivation is as follows: StackOverflow is an important source of documentation. However, different user-provided answers may have very different ratings (based on user votes) based on their perceived value. Therefore, we would like to look at the distribution of questions and their answers. For example, how many highly-rated answers do StackOverflow users post, and how high are their scores? Are there big differences between higher-rated answers and lower-rated ones?\"\n",
    "\n",
    "Data file download link: http://alaska.epfl.ch/~dockermoocs/bigdata/stackoverflow.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Credits to Fahim Sakri \n",
    "# Source (https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d)\n",
    "# An annotation for timing a python function\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print (\"%r  %2.2f ms\" % (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "from post import Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EPFL Wk2 Assignment\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark.conf.set(\"spark.executor.instances\", 1)\n",
    "spark.conf.set(\"spark.executor.cores\", 1)\n",
    "spark.conf.set(\"spark.cores.max\", 1)\n",
    "spark.sparkContext.addPyFile('post.py')\n",
    "\n",
    "# Create RDD\n",
    "data = spark.read.csv('/data/epfl-big-data-analysis/stackoverflow.csv', header=False, inferSchema=True)\n",
    "data = data.withColumnRenamed(\"_c0\", \"post_type_id\") # type 1 = question, type 2 = answer\n",
    "data = data.withColumnRenamed(\"_c1\", \"id\")\n",
    "data = data.withColumnRenamed(\"_c2\", \"acceptedAnswerId\")\n",
    "data = data.withColumnRenamed(\"_c3\", \"parentId\")\n",
    "data = data.withColumnRenamed(\"_c4\", \"score\")\n",
    "data = data.withColumnRenamed(\"_c5\", \"tag\")\n",
    "\n",
    "#data = pd.read_csv('/data/epfl-big-data-analysis/stackoverflow.csv', \n",
    "#                   names=[\"post_type_id\", \"id\", \"acceptedAnswerId\", \"parentId\", \"score\", \"tag\"],\n",
    "#                   dtype={'post_type_id': np.int64, 'score': np.float16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27233496, Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#'))]\n"
     ]
    }
   ],
   "source": [
    "# Preparation - obtain a map of RDD[(QID, Iterable(Question, Answer))]\n",
    "#def groupedPostings(data):\n",
    "questions = posts.filter(lambda p: p.post_type_id == 1).map(lambda p: (p.id, p)).take(1)\n",
    "answers = posts.filter(lambda p: p.post_type_id == 2).map(lambda p: (p.acceptedAnswerId, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " https://github.com/seahrh/stackoverflow-spark\n",
    "        \n",
    " val lines   = sc.textFile(\"src/main/resources/stackoverflow/stackoverflow.csv\")  \n",
    "\n",
    "  val raw     = rawPostings(lines)  \n",
    "\n",
    "  val grouped = groupedPostings(raw)  \n",
    "\n",
    "  val scored  = scoredPostings(grouped)  \n",
    "\n",
    "  val vectors = vectorPostings(scored)\n",
    "    \n",
    "    lines: the lines from the csv file as strings\n",
    "\n",
    "raw: the raw Posting entries for each line\n",
    "\n",
    "grouped: questions and answers grouped together\n",
    "\n",
    "scored: questions and scores\n",
    "\n",
    "vectors: pairs of (language, score) for each question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
