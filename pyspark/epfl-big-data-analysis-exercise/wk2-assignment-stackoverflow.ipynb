{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering StackOverflow Q&A\n",
    "\n",
    "EPFL Big Data Analysis Week 2 Assignment\n",
    "https://www.coursera.org/learn/scala-spark-big-data/home/info\n",
    "\n",
    "\"The overall goal of this assignment is to implement a distributed k-means algorithm which clusters posts on the popular question-answer platform StackOverflow according to their score. Moreover, this clustering should be executed in parallel for different programming languages, and the results should be compared.\n",
    "\n",
    "The motivation is as follows: StackOverflow is an important source of documentation. However, different user-provided answers may have very different ratings (based on user votes) based on their perceived value. Therefore, we would like to look at the distribution of questions and their answers. For example, how many highly-rated answers do StackOverflow users post, and how high are their scores? Are there big differences between higher-rated answers and lower-rated ones?\"\n",
    "\n",
    "Data file download link: http://alaska.epfl.ch/~dockermoocs/bigdata/stackoverflow.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Credits to Fahim Sakri \n",
    "# Source (https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d)\n",
    "# An annotation for timing a python function\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print (\"%r  %2.2f ms\" % (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "from post import Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EPFL Wk2 Assignment\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark.conf.set(\"spark.executor.instances\", 1)\n",
    "spark.conf.set(\"spark.executor.cores\", 1)\n",
    "spark.conf.set(\"spark.cores.max\", 1)\n",
    "spark.sparkContext.addPyFile('post.py')\n",
    "\n",
    "# Create RDD\n",
    "data = spark.read.csv('/data/epfl-big-data-analysis/stackoverflow.csv', header=False, inferSchema=True)\n",
    "data = data.withColumnRenamed(\"_c0\", \"post_type_id\") # type 1 = question, type 2 = answer\n",
    "data = data.withColumnRenamed(\"_c1\", \"id\")\n",
    "data = data.withColumnRenamed(\"_c2\", \"acceptedAnswerId\")\n",
    "data = data.withColumnRenamed(\"_c3\", \"parentId\")\n",
    "data = data.withColumnRenamed(\"_c4\", \"score\")\n",
    "data = data.withColumnRenamed(\"_c5\", \"tag\")\n",
    "\n",
    "#data = pd.read_csv('/data/epfl-big-data-analysis/stackoverflow.csv', \n",
    "#                   names=[\"post_type_id\", \"id\", \"acceptedAnswerId\", \"parentId\", \"score\", \"tag\"],\n",
    "#                   dtype={'post_type_id': np.int64, 'score': np.float16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#posts = data.rdd\n",
    "posts = spark.sparkContext.parallelize(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Preparation\n",
    "Obtain a map that groups the questions and the corresponding answers together.  \n",
    "An RDD suitable for storing this map should be of type RDD[(QID, Iterable(Question, Answer))] For this map, QID is the key and that is the ID of the question post and the value is a iterable collection containing the question post and the answers for that question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = posts.filter(lambda p: p.post_type_id == 1).map(lambda p: (p.id, p))\n",
    "answers = posts.filter(lambda p: p.post_type_id == 2).map(lambda p: (p.parentId, p))\n",
    "posts_map = questions.join(answers).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27233496, Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#'))]\n",
      "[(5484340, Row(post_type_id=2, id=5494879, acceptedAnswerId=None, parentId=5484340, score=1, tag=None))]\n",
      "[(5484340, (Row(post_type_id=1, id=5484340, acceptedAnswerId=None, parentId=None, score=0, tag='C#'), Row(post_type_id=2, id=5494879, acceptedAnswerId=None, parentId=5484340, score=1, tag=None)))]\n"
     ]
    }
   ],
   "source": [
    "# Materialise and check first entry\n",
    "print(questions.take(1))\n",
    "print(answers.take(1))\n",
    "print(posts_map.take(1))"
=======
    "posts = data.rdd"
>>>>>>> f80b65b0473e8c51169c282d288456fd8a06aee6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 6,
>>>>>>> f80b65b0473e8c51169c282d288456fd8a06aee6
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[(5484340, <pyspark.resultiterable.ResultIterable object at 0x7fb4bc8d4cc0>)]\n"
=======
      "[(27233496, Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#'))]\n"
>>>>>>> f80b65b0473e8c51169c282d288456fd8a06aee6
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "grouped = posts_map.groupByKey()\n",
    "print(grouped.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Compute the highest score out of all the answers for each question\n",
    "The type of RDD for this should be \n",
    "`RDD[(Posting, Int)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_max_score(iterable):\n",
    "    max_score = -1\n",
    "    for v1_iterable in iterable:\n",
    "        question = v1_iterable[0]\n",
    "        score = v1_iterable[1].score\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return (question, max_score)\n",
    "scored = grouped.values().map(post_max_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create Vectors for Clustering\n",
    "Create an RDD of type `RDD[(Int, Int)]` containing vectors that can be used in Kmeans clustering.  Each vector is a pair with two components:\n",
    "1. Index of the language multiplied by the `langSpread` factor\n",
    "2. Highest answer score\n",
    "The `langSpread` factor is to ensures posts about different programming languages have at least distance 50000 provided by the `euclideanDist` function."
=======
    "# Preparation - obtain a map of RDD[(QID, Iterable(Question, Answer))]\n",
    "#def groupedPostings(data):\n",
    "questions = posts.filter(lambda p: p.post_type_id == 1).map(lambda p: (p.id, p)).take(1)\n",
    "answers = posts.filter(lambda p: p.post_type_id == 2).map(lambda p: (p.acceptedAnswerId, p))\n",
    "\n"
>>>>>>> f80b65b0473e8c51169c282d288456fd8a06aee6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langSpread = 50000\n",
    "langs = [\"JavaScript\", \"Java\", \"PHP\", \"Python\", \"C#\", \"C++\", \"Ruby\", \"CSS\",\n",
    "      \"Objective-C\", \"Perl\", \"Scala\", \"Haskell\", \"MATLAB\", \"Clojure\", \"Groovy\"]\n",
    "\n",
    "score.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-19dbd2e4cd22>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-19dbd2e4cd22>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://github.com/seahrh/stackoverflow-spark\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " https://github.com/seahrh/stackoverflow-spark\n",
    "        \n",
    " val lines   = sc.textFile(\"src/main/resources/stackoverflow/stackoverflow.csv\")  \n",
    "\n",
    "  val raw     = rawPostings(lines)  \n",
    "\n",
    "  val grouped = groupedPostings(raw)  \n",
    "\n",
    "  val scored  = scoredPostings(grouped)  \n",
    "\n",
    "  val vectors = vectorPostings(scored)\n",
    "    \n",
    "    lines: the lines from the csv file as strings\n",
    "\n",
    "raw: the raw Posting entries for each line\n",
    "\n",
    "grouped: questions and answers grouped together\n",
    "\n",
    "scored: questions and scores\n",
    "\n",
    "vectors: pairs of (language, score) for each question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
