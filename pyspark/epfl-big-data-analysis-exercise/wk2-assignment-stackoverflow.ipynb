{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering StackOverflow Q&A\n",
    "\n",
    "EPFL Big Data Analysis Week 2 Assignment\n",
    "https://www.coursera.org/learn/scala-spark-big-data/home/info\n",
    "\n",
    "\"The overall goal of this assignment is to implement a distributed k-means algorithm which clusters posts on the popular question-answer platform StackOverflow according to their score. Moreover, this clustering should be executed in parallel for different programming languages, and the results should be compared.\n",
    "\n",
    "The motivation is as follows: StackOverflow is an important source of documentation. However, different user-provided answers may have very different ratings (based on user votes) based on their perceived value. Therefore, we would like to look at the distribution of questions and their answers. For example, how many highly-rated answers do StackOverflow users post, and how high are their scores? Are there big differences between higher-rated answers and lower-rated ones?\"\n",
    "\n",
    "Data file download link: http://alaska.epfl.ch/~dockermoocs/bigdata/stackoverflow.csv\n",
    "\n",
    "**WORK IN PROGRESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Credits to Fahim Sakri \n",
    "# Source (https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d)\n",
    "# An annotation for timing a python function\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print (\"%r  %2.2f ms\" % (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "from post import Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EPFL Wk2 Assignment\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark.conf.set(\"spark.executor.instances\", 1)\n",
    "spark.conf.set(\"spark.executor.cores\", 1)\n",
    "spark.conf.set(\"spark.cores.max\", 1)\n",
    "spark.sparkContext.addPyFile('post.py')\n",
    "\n",
    "# Create RDD\n",
    "data = spark.read.csv('/data/epfl-big-data-analysis/stackoverflow.csv', header=False, inferSchema=True)\n",
    "data = data.withColumnRenamed(\"_c0\", \"post_type_id\") # type 1 = question, type 2 = answer\n",
    "data = data.withColumnRenamed(\"_c1\", \"id\")\n",
    "data = data.withColumnRenamed(\"_c2\", \"acceptedAnswerId\")\n",
    "data = data.withColumnRenamed(\"_c3\", \"parentId\")\n",
    "data = data.withColumnRenamed(\"_c4\", \"score\")\n",
    "data = data.withColumnRenamed(\"_c5\", \"tag\")\n",
    "\n",
    "#data = pd.read_csv('/data/epfl-big-data-analysis/stackoverflow.csv', \n",
    "#                   names=[\"post_type_id\", \"id\", \"acceptedAnswerId\", \"parentId\", \"score\", \"tag\"],\n",
    "#                   dtype={'post_type_id': np.int64, 'score': np.float16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper methods for clustering\n",
    "\n",
    "  def euclideanDistanceSum(x1, y1, x2, y2):\n",
    "    dist = np.sqrt(np.square(np.subtract(px, x)) + np.square(np.subtract(py, y)))\n",
    "    return np.sum(dist)\n",
    "\n",
    "  def findClosest(px, py, x, y):\n",
    "    dist = np.sqrt(np.square(np.subtract(px, x)) + np.square(np.subtract(py, y)))\n",
    "    return np.argmin(dist)\n",
    "\n",
    "  /** Average the vectors */\n",
    "  def averageVectors(ps: Iterable[(Int, Int)]): (Int, Int) = {\n",
    "    val iter = ps.iterator\n",
    "    var count = 0\n",
    "    var comp1: Long = 0\n",
    "    var comp2: Long = 0\n",
    "    while (iter.hasNext) {\n",
    "      val item = iter.next\n",
    "      comp1 += item._1\n",
    "      comp2 += item._2\n",
    "      count += 1\n",
    "    }\n",
    "    ((comp1 / count).toInt, (comp2 / count).toInt)\n",
    "  }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(tag='C#'), Row(tag='JavaScript'), Row(tag='Perl'), Row(tag=None), Row(tag='C++'), Row(tag='Groovy'), Row(tag='Objective-C'), Row(tag='CSS'), Row(tag='MATLAB'), Row(tag='Haskell'), Row(tag='Scala'), Row(tag='Clojure'), Row(tag='PHP'), Row(tag='Ruby'), Row(tag='Python'), Row(tag='Java')]\n"
     ]
    }
   ],
   "source": [
    "# Confirm the tag attribute is a single word indicating the language\n",
    "v = data.select('tag').distinct()\n",
    "print(v.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = spark.sparkContext.parallelize(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Preparation - Grouped posts by question\n",
    "First we use the map function to create kv pairs for each type of posts namely questions and answers.  \n",
    "Then a join operation is used for merging the two datasets.  A dataset `RDD[(QID, Iterable(Question, Answer))]` should be useful, the key is the ID of the question post and the values is a collection of tuple (Question, Answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27233496, Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#'))]\n",
      "[(5484340, Row(post_type_id=2, id=5494879, acceptedAnswerId=None, parentId=5484340, score=1, tag=None))]\n",
      "[(5484340, <pyspark.resultiterable.ResultIterable object at 0x7f43298d5780>), (9002525, <pyspark.resultiterable.ResultIterable object at 0x7f43298d5588>)]\n"
     ]
    }
   ],
   "source": [
    "questions = posts.filter(lambda p: p.post_type_id == 1).map(lambda p: (p.id, p))\n",
    "answers = posts.filter(lambda p: p.post_type_id == 2).map(lambda p: (p.parentId, p))\n",
    "grouped = questions.join(answers).groupByKey() # Use inner join to exclude posts with no answers\n",
    "print(questions.take(1))\n",
    "print(answers.take(1))\n",
    "print(grouped.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Calculate maximum answer score for each question\n",
    "Produce a set of key-value pairs - Key of the pair is the question and value should be the maximum answer score of the question.  The output is an `RDD[(Posting, Int)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Row(post_type_id=1, id=5484340, acceptedAnswerId=None, parentId=None, score=0, tag='C#'), 1), (Row(post_type_id=1, id=9002525, acceptedAnswerId=None, parentId=None, score=2, tag='C++'), 4)]\n"
     ]
    }
   ],
   "source": [
    "def post_max_scores(iterable):\n",
    "    max_score = -1\n",
    "    for pair in iterable:\n",
    "        question = pair[0]\n",
    "        answer_score = pair[1].score\n",
    "        if answer_score > max_score:\n",
    "            max_score = answer_score\n",
    "        return (question, max_score)\n",
    "\n",
    "post_scores = grouped.values().map(post_max_scores) #(post_max_score)\n",
    "print(post_scores.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create vectors for clustering\n",
    "Prepare the vectors as an input for clustering.  \n",
    "\n",
    "<br/>\n",
    "Index of the language (in the langs list) multiplied by the `langSpread` factor.\n",
    "\n",
    "The highest answer score (computed above).\n",
    "\n",
    "The `langSpread factor` is provided (set to 50000). Basically, it makes sure posts about different programming languages have at least distance 50000 using the distance measure provided by the euclideanDist function. You will learn later what this distance means and why it is set to this value. The output is `RDD[(Int, Int)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[(200000, 1)]\n"
     ]
    }
   ],
   "source": [
    "langSpread = 50000\n",
    "langs = [\"JavaScript\", \"Java\", \"PHP\", \"Python\", \"C#\", \"C++\", \"Ruby\", \"CSS\",\n",
    "      \"Objective-C\", \"Perl\", \"Scala\", \"Haskell\", \"MATLAB\", \"Clojure\", \"Groovy\"]\n",
    "\n",
    "def as_vectors(iterable):\n",
    "    return \n",
    "#vectors = post_scores.map(as_vectors)\n",
    "vectors = post_scores.map(lambda s: (langs.index(s[0].tag)*langSpread, s[1]))\n",
    "#vectors = post_scores.flatMap(lambda v:v)\n",
    "print(vectors.count())\n",
    "print(vectors.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " https://github.com/seahrh/stackoverflow-spark\n",
    "        \n",
    " val lines   = sc.textFile(\"src/main/resources/stackoverflow/stackoverflow.csv\")  \n",
    "\n",
    "  val raw     = rawPostings(lines)  \n",
    "\n",
    "  val grouped = groupedPostings(raw)  \n",
    "\n",
    "  val scored  = scoredPostings(grouped)  \n",
    "\n",
    "  val vectors = vectorPostings(scored)\n",
    "    \n",
    "    lines: the lines from the csv file as strings\n",
    "\n",
    "raw: the raw Posting entries for each line\n",
    "\n",
    "grouped: questions and answers grouped together\n",
    "\n",
    "scored: questions and scores\n",
    "\n",
    "vectors: pairs of (language, score) for each question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
