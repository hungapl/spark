{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering StackOverflow Q&A\n",
    "\n",
    "EPFL Big Data Analysis Week 2 Assignment\n",
    "https://www.coursera.org/learn/scala-spark-big-data/home/info\n",
    "\n",
    "\"The overall goal of this assignment is to implement a distributed k-means algorithm which clusters posts on the popular question-answer platform StackOverflow according to their score. Moreover, this clustering should be executed in parallel for different programming languages, and the results should be compared.\n",
    "\n",
    "The motivation is as follows: StackOverflow is an important source of documentation. However, different user-provided answers may have very different ratings (based on user votes) based on their perceived value. Therefore, we would like to look at the distribution of questions and their answers. For example, how many highly-rated answers do StackOverflow users post, and how high are their scores? Are there big differences between higher-rated answers and lower-rated ones?\"\n",
    "\n",
    "Data file download link: http://alaska.epfl.ch/~dockermoocs/bigdata/stackoverflow.csv\n",
    "\n",
    "**WORK IN PROGRESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Credits to Fahim Sakri \n",
    "# Source (https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d)\n",
    "# An annotation for timing a python function\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print (\"%r  %2.2f ms\" % (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "from post import Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EPFL Wk2 Assignment\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "    .config(\"spark.cores.max\", 4) \\\n",
    "    .config(\"spark.executor.memory\", \"4G\")\\\n",
    "    .config(\"spark.driver.memory\", \"12G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"10G\")\\\n",
    "    .getOrCreate()\n",
    "        \n",
    "# Create RDD\n",
    "data = spark.read.csv('/data/epfl-big-data-analysis/stackoverflow.csv', header=False, inferSchema=True)\n",
    "data = data.withColumnRenamed(\"_c0\", \"post_type_id\") # type 1 = question, type 2 = answer\n",
    "data = data.withColumnRenamed(\"_c1\", \"id\")\n",
    "data = data.withColumnRenamed(\"_c2\", \"acceptedAnswerId\")\n",
    "data = data.withColumnRenamed(\"_c3\", \"parentId\")\n",
    "data = data.withColumnRenamed(\"_c4\", \"score\")\n",
    "data = data.withColumnRenamed(\"_c5\", \"tag\")\n",
    "\n",
    "#data = pd.read_csv('/data/epfl-big-data-analysis/stackoverflow.csv', \n",
    "#                   names=[\"post_type_id\", \"id\", \"acceptedAnswerId\", \"parentId\", \"score\", \"tag\"],\n",
    "#                   dtype={'post_type_id': np.int64, 'score': np.float16})\n",
    "data_size = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8143801\n"
     ]
    }
   ],
   "source": [
    "data.select('tag').distinct().collect()\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81438\n",
      "[Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#')]\n",
      "[None, 'Java', 'Python', 'JavaScript', 'CSS', 'Groovy', 'C++', 'Objective-C', 'Haskell', 'Ruby', 'Scala', 'MATLAB', 'Clojure', 'PHP', 'Perl', 'C#']\n"
     ]
    }
   ],
   "source": [
    "sample_size = int(data_size/100)\n",
    "print(sample_size)\n",
    "posts = spark.sparkContext.parallelize(data.head(sample_size)).filter(lambda p: p.tag is not None or p.post_type_id == 2)\n",
    "#langs = [\"JavaScript\", \"Java\", \"PHP\", \"Python\", \"C#\", \"C++\", \"Ruby\", \"CSS\",\n",
    "#\"Objective-C\", \"Perl\", \"Scala\", \"Haskell\", \"MATLAB\", \"Clojure\", \"Groovy\"]\n",
    "print(posts.take(1))\n",
    "#posts.filter(lambda p: p.post_type_id == 1)\n",
    "langs = posts.map(lambda p: p.tag).distinct().collect()\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Preparation - Grouped posts by question\n",
    "First we use the map function to create kv pairs for each type of posts namely questions and answers.  \n",
    "Then a join operation is used for merging the two datasets.  A dataset `RDD[(QID, Iterable(Question, Answer))]` should be useful, the key is the ID of the question post and the values is a collection of tuple (Question, Answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27233496, Row(post_type_id=1, id=27233496, acceptedAnswerId=None, parentId=None, score=0, tag='C#'))]\n",
      "[(5484340, Row(post_type_id=2, id=5494879, acceptedAnswerId=None, parentId=5484340, score=1, tag=None))]\n",
      "[(21984912, <pyspark.resultiterable.ResultIterable object at 0x7faf641af9b0>), (12108144, <pyspark.resultiterable.ResultIterable object at 0x7faf641af9e8>)]\n"
     ]
    }
   ],
   "source": [
    "questions = posts.filter(lambda p: p.post_type_id == 1).map(lambda p: (p.id, p))\n",
    "answers = posts.filter(lambda p: p.post_type_id == 2).map(lambda p: (p.parentId, p))\n",
    "grouped = questions.join(answers).groupByKey() # Use inner join to exclude posts with no answers\n",
    "print(questions.take(1));\n",
    "print(answers.take(1));\n",
    "print(grouped.take(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Calculate maximum answer score for each question\n",
    "Produce a set of key-value pairs - Key of the pair is the question and value should be the maximum answer score of the question.  The output is an `RDD[(Posting, Int)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_max_scores(iterable):\n",
    "    max_score = -1\n",
    "    for pair in iterable:\n",
    "        question = pair[0]\n",
    "        answer_score = pair[1].score\n",
    "        if answer_score > max_score:\n",
    "            max_score = answer_score\n",
    "        return (question, max_score)\n",
    "\n",
    "def scoredPostings(question_scores):\n",
    "    return question_scores.values().map(post_max_scores)\n",
    "\n",
    "#scored = scoredPostings(grouped)\n",
    "#print(scored.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create vectors for clustering\n",
    "Prepare the vectors as an input for clustering.  \n",
    "\n",
    "<br/>\n",
    "Index of the language (in the langs list) multiplied by the `langSpread` factor.\n",
    "\n",
    "The highest answer score (computed above).\n",
    "\n",
    "The `langSpread factor` is provided (set to 50000). Basically, it makes sure posts about different programming languages have at least distance 50000 using the distance measure provided by the euclideanDist function. You will learn later what this distance means and why it is set to this value. The output is `RDD[(Int, Int)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "langSpread = 50000\n",
    "\n",
    "def vectorPostings(scores):\n",
    "    # Prepare the vectors as an input for clustering\n",
    "    return scores.map(lambda s: (langs.index(s[0].tag)*langSpread, s[1]))\n",
    "\n",
    "#vectors = vectorPostings(scored)\n",
    "#print(vectors.count())\n",
    "#print(vectors.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper methods for clustering\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def euclideanDistanceSum(v1, v2):\n",
    "    distance_sum = 0\n",
    "    for i in np.arange(0, len(v1)):\n",
    "        vector1 = v1[i]\n",
    "        vector2 = v2[i]\n",
    "        distance_sum = distance_sum + distance.euclidean(vector1, vector2)\n",
    "    return distance_sum\n",
    "\n",
    "def findClosest(p, means):\n",
    "    closest_distance = np.iinfo(np.int32).max\n",
    "    for i in np.arange(0, len(means)):\n",
    "        m = means[i]\n",
    "        dist = distance.euclidean(p, m)\n",
    "        print('i:' + str(dist))\n",
    "        if dist < closest_distance:\n",
    "            closest_i = i\n",
    "            closest_distance = dist\n",
    "    return closest_i\n",
    "\n",
    "def averageVectors(v1, v2):\n",
    "    return ((v1[0] + v2[0])/2, (v1[1] + v2[1])/2)\n",
    "\n",
    "def distance_from_mean(v, means):\n",
    "    mean_index = v[0]\n",
    "    mean_vector = means[mean_index]\n",
    "    return distance.euclidean(v[1], mean_vector)\n",
    "    \n",
    "def averageVectors1(v):\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for vector in v:\n",
    "        sum1 = sum1 + vector1\n",
    "        sum2 = sum2 + vector[1]\n",
    "    return (sum1/len(v), sum2/len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(kmeansEta, kmeansMaxIterations, kmeansKernels, vectors, debug=False):\n",
    "    distinct_vectors = vectors.distinct()\n",
    "    sampleMeansRatio = kmeansKernels/distinct_vectors.count()\n",
    "    means = distinct_vectors.sample(False, sampleMeansRatio).collect()\n",
    "    print(\"Number of means:\" + str(len(means)))\n",
    "    return kmeans_itr(kmeansEta, kmeansMaxIterations, means, vectors, debug)\n",
    "\n",
    "\n",
    "def kmeans_itr(kmeansEta, kmeansMaxIterations, means, vectors, debug=False):    \n",
    "    itr = 1\n",
    "    print('Vectors size:' + str(vectors.count()))\n",
    "    print('Means size:' + str(len(means)))\n",
    "    while itr <= kmeansMaxIterations:\n",
    "        clusters = vectors.map(lambda v: (findClosest(v, means), v)).cache()\n",
    "        #if debug:\n",
    "            #print(\"Clusters assignment:\" + str(clusters.keys().collect()))\n",
    "        error = clusters.map(lambda v: (v[0], distance_from_mean(v, means))) \\\n",
    "                .reduceByKey(lambda v1, v2: v1 + v2) \\\n",
    "                .collect()\n",
    "\n",
    "        averaged = clusters.reduceByKey(lambda v1, v2: averageVectors(v1, v2)).sortByKey()\n",
    "        mm = averaged.values().collect()\n",
    "        if debug:\n",
    "            print(\"New means keys:\" + str(averaged.keys()))\n",
    "\n",
    "        if (len(means) != len(mm)):\n",
    "            raise Exception(\"Number of new means (\" + str(len(mm)) +\") is not the same as the number of old means(\" + str(len(means)) + \")\")\n",
    "\n",
    "        d_sum = euclideanDistanceSum(means, mm)\n",
    "        means = mm\n",
    "        \n",
    "        # Calculate distance from mean for each vector\n",
    "        clusters_mean = clusters.join(mm).collect()\n",
    "        \n",
    "        \n",
    "        print(\"Iteration \" + str(itr) + ' completed, convergence=' + str(d_sum))\n",
    "\n",
    "        if d_sum <= kmeansEta:\n",
    "            break\n",
    "\n",
    "        itr = itr + 1\n",
    "        \n",
    "    if debug:\n",
    "        print('means:')\n",
    "        print(means)\n",
    "        print('error:')\n",
    "        print(error)\n",
    "        print('euclideanDistanceSum:' + str(d_sum))\n",
    "    \n",
    "    if itr > kmeansMaxIterations:\n",
    "        print('Failed to converge after ' + str(kmeansMaxIterations) + ' iterations.')\n",
    "    else:\n",
    "        print('Converged after ' + str(itr-1) + ' iterations. Error=' + str(d_sum))\n",
    "    return {'means':means, 'clusters':clusters.collect()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kmeans\n",
    "def testKmeans():\n",
    "    \n",
    "    # An example online\n",
    "    v1 = [18, 21, 22, 24, 26, 26, 27, 30, 31, 35, 39, 40, 41, 42, 44, 46, 47, 48, 49, 54]\n",
    "    v2 = [10, 11, 22, 15, 12, 13, 14, 33, 39, 37, 44, 27, 29, 20, 28, 21, 30, 31, 23, 24]\n",
    "\n",
    "    vv = list()\n",
    "    for ii in np.arange(0, len(v1)):\n",
    "        vv.append((v1[ii], v2[ii]))\n",
    "\n",
    "    my_vectors = spark.sparkContext.parallelize(vv)    \n",
    "    my_means = my_vectors.takeSample(False, 3)\n",
    "\n",
    "    #kmeans(1, 10, my_means, my_vectors)\n",
    "    \n",
    "    kmeans(20, 120, my_means, my_vectors, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means parameter: Number of clusters\n",
    "kmeansKernels = 45\n",
    "## K-means parameter: Convergence criteria\n",
    "kmeansEta = 20\n",
    "## K-means parameter: Maximum iterations\n",
    "kmeansMaxIterations = 130\n",
    "\n",
    "def cluster(data):\n",
    "    my_vectors = vectorPostings(scoredPostings(data))\n",
    "    kmeans(kmeansEta, kmeansMaxIterations, 45, my_vectors)\n",
    "    \n",
    "# Cluster a small sample \n",
    "sample = spark.sparkContext.parallelize(grouped.takeSample(False, 20000))\n",
    "vectors = vectorPostings(scoredPostings(sample))\n",
    "distinct_vectors = vectors.distinct()\n",
    "sampleMeansRatio = kmeansKernels/distinct_vectors.count()\n",
    "means = distinct_vectors.sample(False, sampleMeansRatio).collect()\n",
    "#kmeans_itr(kmeansEta, kmeansMaxIterations, means, vectors, True)\n",
    "clusters = vectors.map(lambda v: (findClosest(v, means), v)).cache()\n",
    "averaged = clusters.reduceByKey(lambda v1, v2: averageVectors(v1, v2)).sortByKey()\n",
    "#results = kmeans(sample)\n",
    "\n",
    "#spark.sparkContext.parallelize(grouped.takeSample(False, 20000))\n",
    "#distinct_vectors = vectors.distinct()\n",
    "#sampleMeansRatio = kmeansKernels/distinct_vectors.count()\n",
    "#means = distinct_vectors.sample(False, sampleMeansRatio).collect()\n",
    "\n",
    "#results = cluster(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, (100000, 2))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clusters.keys().take(1))\n",
    "print(averaged.keys().take(1))\n",
    "merged = clusters.join(averaged)\n",
    "#merged.take(2)\n",
    "merged.mapValues(lambda v : v[0]).take(1).apply(lambda k: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 17750214.891628295),\n",
       " (1, 7942297.838934626),\n",
       " (2, 935.8513609692402),\n",
       " (3, 10132891.141639356),\n",
       " (4, 844.0286521818489),\n",
       " (5, 895.1356080605121),\n",
       " (6, 42.196533203125)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['errors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these initial means, and the provided variables converged method, implement the K-means algorithm by iteratively:\n",
    "\n",
    "pairing each vector with the index of the closest mean (its cluster);\n",
    "\n",
    "computing the new means by averaging the values of each cluster.\n",
    "\n",
    "To implement these iterative steps, use the provided functions findClosest, averageVectors, and euclideanDistance.\n",
    "\n",
    "Note 1:\n",
    "\n",
    "In our tests, convergence is reached after 44 iterations (for langSpread=50000) and in 104 iterations (for langSpread=1), and for the first iterations the distance kept growing. Although it may look like something is wrong, this is the expected behavior. Having many remote points forces the kernels to shift quite a bit and with each shift the effects ripple to other kernels, which also move around, and so on. Be patient, in 44 iterations the distance will drop from over 100000 to 13, satisfying the convergence condition.\n",
    "\n",
    "If you want to get the results faster, feel free to downsample the data (each iteration is faster, but it still takes around 40 steps to converge):\n",
    "\n",
    "val scored  = scoredPostings(grouped).sample(true, 0.1, 0)\n",
    "However, keep in mind that we will test your assignment on the full data set. So that means you can downsample for experimentation, but make sure your algorithm works on the full data set when you submit for grading.\n",
    "\n",
    "Note 2:\n",
    "\n",
    "The variable langSpread corresponds to how far away are languages from the clustering algorithm's point of view. For a value of 50000, the languages are too far away to be clustered together at all, resulting in a clustering that only takes scores into account for each language (similarly to partitioning the data across languages and then clustering based on the score). A more interesting (but less scientific) clustering occurs when langSpread is set to 1 (we can't set it to 0, as it loses language information completely), where we cluster according to the score. See which language dominates the top questions now?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
